%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Proposal: Rank Reasons for Predicting Convincingness of Web Arguments}

\author{Yanan Xie \\
Computer Science Department\\
University of California, Santa Cruz\\
  {\tt yaxie@ucsc.edu} \\\And
  Ziqiang Wang \\
Computer Engineering Department\\
University of California, Santa Cruz\\
  {\tt zwang232@ucsc.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Recently, researchers started to study the problem of predicting convincingness of web arguments. However, existing works all stop by taking advantage of superficial features without considering context and domain knowledge. Thus, those arguments containing weak supporting reasons but composed with convincing words may be automatically classified as highly convincing in existing models. We propose a new method which employs reason level feature to further improve the performance of predicting convincingness of web arguments.
\end{abstract}

\section{Introduction}

The main goal of argumentation is persuation\cite{nettel2012persuasive, mercier2011humans, blair2012argumentation}. Psychology researchers often identify that there are three factors(i.e., the argument itself, the audience of argument, the source of argument ) that affect argument persuasiveness\cite{petty1986elaboration}. 

As an important part of predicting persuasiveness of argument itself, the task of predicting convincingness started to attrack researchers to work on this issue\cite{habernal2016argument}. However, existing works all stop by taking advantage of superficial features without considering context and domain knowledge. They focus on wether the argument gives exmaples, actual reasons and facts rather than how good the examples, reasons, facts are. Thus, those non-sense arguments containing weak or even faked supporting reasons but composed with convincing words may be automatically classified as highly convincing in existing models. In Table \ref{table:argumentexamples}, we give an exmaple of argument pair to show that argument with reason(Argument 1) can be less convincing than one without reasons(Argument 2). It's critical to differentiate reasons of different quality while predicting convincingness. 

\begin{table}[h]
\begin{tabular}{| p{3.5cm} | p{3.5cm} | }
\hline
{\bf Argument 1} & {\bf Argument 2} \\
\hline
YES, because some children donâ€™t understand anything excect physical education especially rich children of rich parents. & Of course!  According to a research in 2018, no one can ever find his or her true love without taking physical education.\\
\hline

\end{tabular}
\caption{Argument examples. {\bf Prompt:} Should physical education be mandatory in schools? {\bf Stance:} Yes!} 
\label{table:argumentexamples}
\end{table}


In order to rank reasons mentioned in arguments, we propose two hypotheses (A) arguments with good reasons are more convincing than those without, and (B) people tend to use more convincing reasons to compose an argument. To verify those hypotheses, we propose a new method which employs reason level feature to further improve the performance of predicting convincingness of web arguments.

The method takes 4 steps. The first step is to identify text segments corresponding to reasons from arguments. Figure \ref{figure:reasonidentification} shows an exmaple of reason identification. Then we use clustering algorithm to group those identified reasons as the second step. With the grouped reasons, we can build an undirected graph where vertices are the arguments. If two arguments share reasons in the same group, we add an edge between two corresponding vertices. We can run a PageRank-like algorithm to score reason strength for all arguments. The last step is to combine reason strength with superficial features with an supervised learning classifier to produce the final score for each argument.  

\begin{figure}

{\it I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.} Let us take a look from the social perspective. {\it If parents cannot afford to provide for the child, or if the family is facing financial constraints, it is understandable that abortion can remain as one of the options.}


\caption{An example of reason identification. Text segments that are identified as reasons are {\it italicized}.} 
\label{figure:reasonidentification}
\end{figure}


\section{Related work}

Predicting convincingness of web arguments as a new research task has recently been studied\cite{habernal2016argument}. High quality human annotated convincingness data of argument pair in multiple domains is also available for research\cite{habernal2016argument}. 

The classification of argument components is to classify text segments in given arguments into two types - claim and premise(reason)\cite{stab2014argumentation}. This issue has been studied by \cite{rooney2012applying,feng2011classifying,palau2009argumentation}. 

Reason classification has recently been studied\cite{hasan2014you}. And human annotated data is available which can also be used for evaluating reason clustering.

\section{Dataset}

The ground truth for convincingness we choose is from \cite{habernal2016argument}. The dataset contains 16,081 argument paris from 32 different topics. 

To train our argument component classifier, we are going to use the data from \cite{hasan2014you} which contains 4 different topics with labeled reasons. However, the only overlapping topic between two datasets is gay marriage. So we will try to train a close-domain model on gay marriage topic and a open-domain model. 

\section{Our method}

Our proposed method identifies text segments expressing reasons from argument as the first step. An undirected graph based on similarities between reason pairs will be built for each topic. Then we use a PageRank-like algorithm to rank reasons from the generated graph while the final step is to combine reason scores with lexicon features from argument to produce the convincingness score for each argument.  

\subsection{Reason identification}

Reason identification usually take advantage of discourse markers\cite{palau2009argumentation}. However, it has been reported that only few of reasons include discourse markers\cite{marcu2002unsupervised}. Given enough arguments in a topic, we can use a model based on discourse markers to select a small part of reasons, and then use reason classification techniques to identify reasons that include no discourse markers.

\subsection{Reason similarity}

Word alignment has been proven to be a powerful method to measure semantic similarity between two sentences\cite{sultan2015dls}. We are going to use a similar technique to measure similarity between two text segements identified as reasons.

\subsection{Ranking algorithm}

With the undirected graph generated with similarity between reasons from two arguments, we can perform a PageRank-like algorithm to further score arguments in terms of their reasons\cite{grolmusz2015note}. 

\subsection{Combining features}

We can easily add the computed reason scores into the feature set of \cite{habernal2016argument} and use the same classifier, Support Vector Machine to combine reason level feature with lexicon features. 


\section{Evaluation}

With the pairwise dataset, we can first evaluate our method in pair to see the precision. Also, we can further evaluate the convincingness order among given arguments by how many human labeled pair evaluations can match the algorithm produced order. 



% include your own bib file like this:
% \bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{acl2017}
\bibliographystyle{acl_natbib}


\end{document}
